# -*- coding: utf-8 -*-
"""AI LAB .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DVlG0f03ps8mqo_W5itX7kMhIej1JmDu

EXP 1 CAMel Banana
"""

dp = [[-1 for i in range(3001)] for j in range(1001)]
 
# Recursive function to find the maximum
# number of bananas that can be transferred
# to A distance using memoization
def recBananaCnt(A, B, C):
 
    # Base Case where count of bananas
    # is less that the given distance
    if (B <= A):
        return 0
         
    # Base Case where count of bananas
    # is less that camel's capacity
    if (B <= C):
        return B - A
     
    # Base Case where distance = 0
    if (A == 0):
        return B
     
 
    # If the current state is already
    # calculated
    if (dp[A][B] != -1):
        return dp[A][B]
     
 
    # Stores the maximum count of bananas
    maxCount = -2**32
 
    # Stores the number of trips to transfer
    # B bananas using a camel of capacity C
    tripCount = ((2 * B) // C) - 1 if(B % C == 0 ) else ((2 * B) // C) + 1
 
    # Loop to iterate over all the
    # breakpoints in range [1, A]
    for i in range(1,A+1):
 
        # Recursive call over the
        # remaining path
        curCount = recBananaCnt(A - i, B - tripCount * i, C)
 
        # Update the maxCount
        if (curCount > maxCount):
            maxCount = curCount
 
            # Memoize the current value
            dp[A][B] = maxCount
         
    # Return answer
    return maxCount
 
# Function to find the maximum number of
# bananas that can be transferred
def maxBananaCnt(A, B, C):
 
    # Function Call
    return recBananaCnt(A, B, C)
 
# Driver Code
A = 1000
B = 3000
C = 1000
print("Maximum Bananas the camel can carry",maxBananaCnt(A, B, C))

"""EXP 2 Graph color"""

# A function to print the color configuration.
def printConfiguration(colorArray):
    print("The assigned colors are as follows:")
    for i in range(4):
        print("Vertex: ",
              i, " Color: ", colorArray[i])


# function to check if graph valid
def isSafe(graph, colorArray):
    for i in range(4):
        for j in range(i + 1, 4):
            if (graph[i][j] and colorArray[j] == colorArray[i]):
                return False
    return True



#recursive function to return true and false if the color m can be applied to vertex i
def graphColoringAlgorithm(graph, m, i, colorArray):
    # If we have reached the last vertex then check and print the configuration.
    if (i == 4):
        if (isSafe(graph, colorArray)):
            printConfiguration(colorArray)
            return True
        return False

    # Assigning color to the vertex and recursively calling the function.
    for j in range(1, m + 1):
        colorArray[i] = j
        if (graphColoringAlgorithm(graph, m, i + 1, colorArray)):
            return True
        colorArray[i] = 0
    return False


if __name__ == '__main__':
    graph = [
        [0, 1, 1, 1],
        [1, 0, 1, 0],
        [1, 1, 0, 1],
        [1, 0, 1, 0],
    ]
    m = 3

    # Initially the color list is initialized with 0.
    colorArray = [0 for i in range(4)]

    if (graphColoringAlgorithm(graph, m, 0, colorArray)):
        print("Coloring is possible!")
    else:
        print("Coloring is not possible!")

"""COnstraint SSatisfaction problem


"""

import re

solved = False

#recursive function
def solve(letters, values, visited, words):
  global solved
  if len(set) == len(values):
    map = {}
    for letter, val in zip(letters,values):
      map[letter] = val

    if map[words[0][0]] == 0 or map[words[1][0]] == 0 or map[words[2][0]] == 0:
      return

    word1, word2, res = "", "", ""
    #making possiblew combinations with current knowledge
    for c in words[0]:
      word1 += str(map[c])
    for c in words[1]:
      word2 += str(map[c])
    for c in words[2]:
      res += str(map[c])
    if int(word1) + int(word2) == int(res):
      print("{} + {} = {}\t{}".format(word1, word2, res, map))
      solved = True
      return

  
  for i in range(10):
      if not visited[i]:
        visited[i] = True
        values.append(i)
        solve(letters, values, visited, words)
        values.pop()
        visited[i] = False

print("\nCRYPTARITHMETIC PUZZLE SOLVER")
print("WORD1 + WORD2 = RESULT")
word1 = input("Enter WORD1: ").upper()
word2 = input("Enter WORD2: ").upper()
result = input("Enter RESULT: ").upper()

if len(result) > (max(len(word1), len(word2)) + 1):
  print("\n0 Solutions!")
else:
  set = []
  for c in word1:
    if c not in set:
      set.append(c)
  for c in word2:
    if c not in set:
      set.append(c)
  for c in result:
    if c not in set:
      set.append(c)

  if len(set) > 10:
    print("\nNo solutions!")
    exit()

  print("Solutions:")
  solve(set, [], [False for _ in range(10)], [word1, word2, result])

  if not solved:
    print("\n0 solutions!")

"""BFS : Water Jug"""

graph = {
  '5' : ['3','7'],
  '3' : ['2', '4'],
  '7' : ['8'],
  '2' : [],
  '4' : ['8'],
  '8' : []
}

visited = [] # List for visited nodes.
queue = []     #Initialize a queue

def bfs(visited, graph, node): #function for BFS
  visited.append(node)
  queue.append(node)

  while queue:          # Creating loop to visit each node
    m = queue.pop(0) 
    print (m, end = " ") 

    for neighbour in graph[m]:
      if neighbour not in visited:
        visited.append(neighbour)
        queue.append(neighbour)

# Driver Code
print("Following is the Breadth-First Search")
bfs(visited, graph, '5')    # function calling

graph ={'1':['2','3'],'2':['5'],'3':['7'],'5':[],'7':[]}

visited = []
queue = []
def bfs(visited,graph,node):
  visited.append(node)
  queue.append(node)

  while queue:
    m = queue.pop(0)
    print(m , end =" ")

    for neighbour in graph[m]:
      if neighbour not in visited:
        visited.append(neighbour)
        queue.append(neighbour)



visited2 = set()
def dfs(visited,graph,node):
  if node not in visited2:
    print(node,end=" ")
    visited2.add(node)

  for n in graph[node]:
    dfs(visited,graph,n)  


bfs(visited,graph,'1')     
print()   
dfs(visited2,graph,'1')

# Import libraries
from urllib.request import urljoin
from bs4 import BeautifulSoup
import requests
from urllib.request import urlparse


# Set for storing urls with same domain
links_intern = set()
input_url = "https://www.geeksforgeeks.org/machine-learning/"
depth = 1

# Set for storing urls with different domain
links_extern = set()


# Method for crawling a url at next level
def level_crawler(input_url):
	temp_urls = set()
	current_url_domain = urlparse(input_url).netloc

	# Creates beautiful soup object to extract html tags
	beautiful_soup_object = BeautifulSoup(
		requests.get(input_url).content, "lxml")

	# Access all anchor tags from input
	# url page and divide them into internal
	# and external categories
	for anchor in beautiful_soup_object.findAll("a"):
		href = anchor.attrs.get("href")
		if(href != "" or href != None):
			href = urljoin(input_url, href)
			href_parsed = urlparse(href)
			href = href_parsed.scheme
			href += "://"
			href += href_parsed.netloc
			href += href_parsed.path
			final_parsed_href = urlparse(href)
			is_valid = bool(final_parsed_href.scheme) and bool(
				final_parsed_href.netloc)
			if is_valid:
				if current_url_domain not in href and href not in links_extern:
					print("Extern - {}".format(href))
					links_extern.add(href)
				if current_url_domain in href and href not in links_intern:
					print("Intern - {}".format(href))
					links_intern.add(href)
					temp_urls.add(href)
	return temp_urls


if(depth == 0):
 print('0')
 print("Intern - {}".format(input_url))

elif(depth == 1):
 print('1')
 level_crawler(input_url)

else:
	# We have used a BFS approach
	# considering the structure as
	# a tree. It uses a queue based
	# approach to traverse
	# links upto a particular depth.
	queue = []
	queue.append(input_url)
	for j in range(depth):
		for count in range(len(queue)):
			url = queue.pop(0)
			urls = level_crawler(url)
			for i in urls:
				queue.append(i)

"""DFS

"""

# Using a Python dictionary to act as an adjacency list
graph = {
  '5' : ['3','7'],
  '3' : ['2', '4'],
  '7' : ['8'],
  '2' : [],
  '4' : ['8'],
  '8' : []
}

visited = set() # Set to keep track of visited nodes of graph.

def dfs(visited, graph, node):  #function for dfs 
    if node not in visited:
        print (node)
        visited.add(node)
        for neighbour in graph[node]:
            dfs(visited, graph, neighbour)

# Driver Code
print("Following is the Depth-First Search")
dfs(visited, graph, '5')



"""**EXP 5 BEST FIRST**"""

SuccList ={ 'A':[['B',3],['C',2]], 'B':[['A',5],['C',2],['D',2],['E',3]], 'C':[['A',5],['B',3],['F',2],['G',4]], 'D':[['H',1],['I',99]],'F': [['J',99]],'G':[['K',99],['L',3]]}
Start='A'
Goal='E'
Closed = list()
SUCCESS=True
FAILURE=False
State=FAILURE

def MOVEGEN(N):
	New_list=list()
	if N in SuccList.keys():
		New_list=SuccList[N]
	
	return New_list
	
def GOALTEST(N):
	if N == Goal:
		return True
	else:
		return False

def APPEND(L1,L2):
	New_list=list(L1)+list(L2)
	return New_list
	
def SORT(L):
	L.sort(key = lambda x: x[1]) 
	return L 
	
def BestFirstSearch():
	OPEN=[[Start,5]]
	CLOSED=list()
	global State
	global Closed
	while (len(OPEN) != 0) and (State != SUCCESS):
		print("------------")
		N= OPEN[0]
		print("N=",N)
		del OPEN[0] #delete the node we picked
		
		if GOALTEST(N[0])==True:
			State = SUCCESS
			CLOSED = APPEND(CLOSED,[N])
			print("CLOSED=",CLOSED)
		else:
			CLOSED = APPEND(CLOSED,[N])
			print("CLOSED=",CLOSED)
			CHILD = MOVEGEN(N[0])
			print("CHILD=",CHILD)
			for val in CLOSED:
				if val in CHILD:
					CHILD.remove(val)
			for val in OPEN:
				if val in CHILD:
					CHILD.remove(val)
			OPEN = APPEND(CHILD,OPEN) #append movegen elements to OPEN
			print("Unsorted OPEN=",OPEN)
			SORT(OPEN)
			print("Sorted OPEN=",OPEN)
			
	Closed=CLOSED
	return State
	
#Driver Code
result=BestFirstSearch() #call search algorithm
print(Closed,result)

n = int(input())

G = list()

for i in (0,n):
  x = input()
  y = input()
  z = int(input())
  G = list.append([x,y,z])

print (G)

from queue import PriorityQueue
import matplotlib.pyplot as plt
import networkx as nx

# for implementing BFS | returns path having lowest cost
def best_first_search(source, target, n):
    visited = [0] * n
    visited[source] = True
    pq = PriorityQueue()
    pq.put((0, source))
    while pq.empty() == False:
        u = pq.get()[1]
        print(u, end=" ") # the path having lowest cost
        if u == target:
            break

        for v, c in graph[u]:
            if visited[v] == False:
                visited[v] = True
                pq.put((c, v))
    print()

# for adding edges to graph
def addedge(x, y, cost):
    graph[x].append((y, cost))
    graph[y].append((x, cost))

G = nx.Graph()
v = int(input("Enter the number of nodes: "))
graph = [[] for i in range(v)] # undirected Graph
e = int(input("Enter the number of edges: "))
print("Enter the edges along with their weights:")
for i in range(e):
    x, y, z = list(map(int, input().split()))
    addedge(x, y, z)
    G.add_edge(x, y, weight = z)

source = int(input("Enter the Source Node: "))
target = int(input("Enter the Target/Destination Node: "))

##Graph function
print("Graph:\n")
pos = nx.spring_layout(G, seed=7)  # positions for all nodes - seed for reproducibility
color_map = ["yellow" if (node==0 or node==1 or node==2 or node==3 or node==8 or node==9) else "blue" for node in G]


# nodes
nx.draw_networkx_nodes(G, pos, node_size=350,node_color=color_map)

# edges
nx.draw_networkx_edges(G, pos)
nx.draw_networkx_edges(G, pos, alpha=0.5, edge_color="r")

# labels
nx.draw_networkx_labels(G, pos, font_size=20)

ax = plt.gca()
ax.margins(0.08)
plt.axis("off")
plt.tight_layout()
plt.show()


print("\nPath: ", end = "")
best_first_search(source, target, v)

"""**EXP 5 A* SEARCH**"""

!pip install pyamaze

import sys
import random
import numpy as np
import heapq 
SIZE = (9,9 )
import heapq as hq
import time
# size of the labyrinth (x, y)


if sys.getrecursionlimit() < SIZE[0] * SIZE[1]:
    sys.setrecursionlimit(SIZE[0] * SIZE[1])
# if max recursion limit is lower than needed, adjust it

N, S, E, W = 1, 2, 4, 8
# directions translated into bitnums to store information on all cleared walls in one variable per cell

GO_DIR = {N: (0, -1), S: (0, 1), E: (1, 0), W: (-1, 0)}
# dictionary with directions translated to digging moves

REVERSE = {E: W, W: E, N: S, S: N}
# when a passage is dug from a cell, the other cell obtains the reverse passage, too

lab = list(list(0 for i in range(SIZE[0])) for j in range(SIZE[1]))

def dig(x, y):
    # digs passage from a cell (x, y) in an unvisited cell
    dirs = [N, E, W, S]
    random.shuffle(dirs)
    # shuffles directions each time for more randomness
    for dir in dirs:
        new_x = x + GO_DIR[dir][0]
        new_y = y + GO_DIR[dir][1]
        if (new_y in range(SIZE[1])) and\
        (new_x in range(SIZE[0])) and\
        (lab[new_y][new_x] == 0):
            # checks if the new cell is not visited
            lab[y][x] |= dir
            lab[new_y][new_x] |= REVERSE[dir]
            # if so, apply info on passages to both cells
            dig(new_x, new_y)
            # repeat recursively
    
def check():
    # displays the cells' values for check-up
    for i in range(SIZE[1]):
        for j in range(SIZE[0]):
            print(" "*(1-(lab[i][j]//10))+\
            str(lab[i][j]), end='|')
        print ('')


def create():
    # displays the labyrinth
    # prints the seed (for reference) and the lab size‚
    for k in range(SIZE[0]):
        print(" ", end='')
        print(k, end='')
    myArray =[]
    line = []
    #print()
    #print("_" * (SIZE[0] * 2))
    
    
    for i in range (0,SIZE[0] * 2+1):
        line.append("_")
        
    myArray.append(line)
    for j in range(SIZE[1]):
        line =[]
        if j!=0:
            #print("|", end='')
            line.append("|")
        else:
            #print ("|", end='')
            line.append("|")
        for i in range(SIZE[0]):
            if (lab[j][i] & S != 0):
                #print(" ", end='')
                line.append(" ")
            else:
                #print("_", end='')
                line.append("_")
            if (lab[j][i] & E != 0):
                if ((lab[j][i] | lab[j][i+1]) & S != 0):
                    #print(" ", end='')
                    line.append(" ")
                else:
                    #print("_", end='')
                    line.append("_")
            elif (j==SIZE[1]-1) & (i==SIZE[0]-1):
                #print("|", end='')
                line.append("|")
            else:
                #print("|", end='')
                line.append("|")
        #print(j)
        myArray.append(line)
        
    return myArray
    
#myArray=np.chararray((SIZE[0],SIZE[1]))

def printArray(myArray):
    #for k in range(SIZE[0]):
        #print("_"+str(k)+"_", end='')
    for i in range(1, 24):
        print(str(i)+" ", end='')
        
        
    print()
    print()    
    
        
    for j in range (0,len(myArray)):
        for i in range (0, len(myArray[1])):
            print(myArray[j][i], end='')
        #print(j)
        print(" "+ str(j))



def Heuristic(DesiredPoint, CurrentLocation):
    return np.linalg.norm(DesiredPoint-CurrentLocation)


def newMatrix(myArray):
    line = []
    myArray1 = []
    for j in range (0,len(myArray)*3):
        line = []
        for i in range (0, len(myArray[0])*3):
            line.append(" ")
        myArray1.append(line)
        
    for j in range (1,len(myArray)+1):
        for i in range (1, len(myArray[1])+1):
            #print(i)
           # print(j)
            if (myArray[j-1][i-1] == "_"):
                myArray1[j*3-3][i*3-1] ="_"
                myArray1[j*3-3][i*3-2] ="_"
                myArray1[j*3-3][i*3-3] ="_"
                for c in range (j*3-2,j*3):
                    for b in range(i*3-3,i*3):
                        myArray1[c][b] = " "
            else :
                for c in range(j*3-3,j*3):
                    for b in range(i*3-3,i*3):
                        myArray1[c][b] = " "
                        
                        
    for j in range (1,len(myArray)+1):
        for i in range (1, len(myArray[1])+1):
            #print(i)
            #print(j)
            if (myArray[j-1][i-1] == "|"):
                myArray1[j*3-3][i*3-3] ="|"
                myArray1[j*3-4][i*3-3] ="|"
                myArray1[j*3-5][i*3-3] ="|"
                for c in range (j*3-3,j*3):
                    for b in range(i*3-2,i*3):
                        myArray1[c][b] = " "
            else :
                for c in range(j*3-3,j*3):
                    for b in range(i*3-3,i*3):
                        if (myArray1[c][b]== " "):
                            myArray1[c][b] = " "
                
                
    return myArray1
                
        
    
            
    
    #newmatrix =np.zeros((len(myArray[0])*2,len(myArray[1])*2))
    


        
seed = random.randint(0, 1000)
random.seed(seed)
dig(SIZE[0]//2, SIZE[1]//2)
# create matrix
myArray = create()
# change size
myArray = newMatrix(myArray)
printArray(myArray)
print("Please enter your starting x1 y1")
x1 = int(input())
y1 = int(input())
print("Please enter your desired location")
x2 = int(input())
y2 = int(input())

myArray[x1][y1] = "X"
myArray[x2][y2] = "Y"
printArray(myArray)

# Route
CurrentLocation = np.array([x1, y1])
DesiredPoint =  np.array([x2,y2])
Move = np.array([[-1,0],  #go up 
        [0,-1], #go left  
        [1,0],  #go down
        [0, 1]]) #go right

G = 0
Astart = G

heap = []
while not((DesiredPoint ==CurrentLocation)[0] and (DesiredPoint ==CurrentLocation)[1]):
    G = G+0.01
    #Heaparray = []
    Position0 = CurrentLocation + Move[0]
    Position1 = CurrentLocation + Move[1]
    Position2 = CurrentLocation + Move[2]
    Position3 = CurrentLocation + Move[3]
    if  Position0[0] != 0  and Position0[1] != 0 and Position0[0] != len(myArray) and Position0[0] != len(myArray[1]) and myArray[Position0[0]][Position0[1]] != "|"  and myArray[Position0[0]][Position0[1]] != "_":
        heuristic0 = Heuristic(DesiredPoint, Position0)
        Astar0 = G + heuristic0
        hq.heappush(heap, (Astar0,(str(Position0[0])+"-"+str(Position0[1]))))
    if  Position1[0] != 0  and Position1[1] != 0 and Position1[0] != len(myArray) and Position1[0] != len(myArray[1])  and myArray[Position1[0]][Position1[1]]  != "_" and myArray[Position1[0]][Position1[1]]  != "|":
        heuristic1 = Heuristic(DesiredPoint, Position1)
        Astar1 = G + heuristic1
        hq.heappush(heap, (Astar1,(str(Position1[0])+"-"+str(Position1[1]))))
        
    if  Position2[0] != 0  and Position2[1] != 0 and Position2[0] != len(myArray) and Position2[0] != len(myArray[1]) and myArray[Position2[0]][Position2[1]] != "|" and   myArray[Position2[0]][Position2[1]] != "_" :
        heuristic2 = Heuristic(DesiredPoint, Position2)
        Astar2 = G + heuristic2
        hq.heappush(heap, (Astar2,(str(Position2[0])+"-"+str(Position2[1]))))
   
    
    Position3 = CurrentLocation + Move[3]
    if  Position3[0] != 0  and Position3[1] != 0 and Position3[0] != len(myArray) and Position3[0] != len(myArray[1])  and myArray[Position3[0]][Position3[1]]  != "_" and myArray[Position3[0]][Position3[1]]  != "|" :
        heuristic3 = Heuristic(DesiredPoint, Position2)
        Astar3 = G + heuristic3
        hq.heappush(heap, (Astar3,(str(Position3[0])+"-"+str(Position3[1]))))
        
    #print(G)  
    smallest = heapq.heappop(heap)
    #heapq.heapify(heap)
    xy = smallest[1]
    Location = xy.index("-")
    x = int(xy[0:Location])
    y = int(xy[Location+1:])
    CurrentLocation = np.array([x ,y])
    myArray[x][y] = "X"
    #printArray(myArray)
    #time.sleep(0.3)
    # add them on the stack or que 
    # take the lowest cost from que and expolore this point
    if (len(heap)>5000):
        cheap = []
        for i in range(0,1000):
            hq.heappush(cheap,heapq.heappop(heap))
        heap = cheap
    
print(G)
printArray(myArray)
print("please select your starting point and the point you want to reach")

"""EXP - 6 DST FUZZY LOGIC"""

A = dict()
B = dict()
Y = dict()
X = dict()
A = {"a":0.2,"b":0.3,"c":0.6,"d":0.6}
B = {"a":0.9,"b":0.9,"c":0.4,"d":0.5}
print('The First Fuzzy Set is', A)
print('The Second Fuzzy Set is', B)
for A_key, B_key in zip(A, B):
  A_value = A[A_key]
  B_value = B[B_key]
  if A_value > B_value:
    Y[A_key] = A_value
  else:
    Y[B_key] = B_value
print('Fuzzy Set Union is', Y)
# Difference Between Two Fuzzy Sets
for A_key in A:
  X[A_key]= 1-A[A_key]
print('Fuzzy Set Complement is', X)

"""**EXP -7 UNIFICATION AND RESOLUTION**"""

def get_index_comma(string):
    index_list = list()
    par_count = 0

    for i in range(len(string)):
        if string[i] == ',' and par_count == 0:
            index_list.append(i)
        elif string[i] == '(':
            par_count += 1
        elif string[i] == ')':
            par_count -= 1

    return index_list


def is_variable(expr):
    for i in expr:
        if i == '(' or i == ')':
            return False

    return True


def process_expression(expr):
    expr = expr.replace(' ', '')
    index = None
    for i in range(len(expr)):
        if expr[i] == '(':
            index = i
            break
    predicate_symbol = expr[:index]
    expr = expr.replace(predicate_symbol, '')
    expr = expr[1:len(expr) - 1]
    arg_list = list()
    indices = get_index_comma(expr)

    if len(indices) == 0:
        arg_list.append(expr)
    else:
        arg_list.append(expr[:indices[0]])
        for i, j in zip(indices, indices[1:]):
            arg_list.append(expr[i + 1:j])
        arg_list.append(expr[indices[len(indices) - 1] + 1:])

    return predicate_symbol, arg_list


def get_arg_list(expr):
    _, arg_list = process_expression(expr)

    flag = True
    while flag:
        flag = False

        for i in arg_list:
            if not is_variable(i):
                flag = True
                _, tmp = process_expression(i)
                for j in tmp:
                    if j not in arg_list:
                        arg_list.append(j)
                arg_list.remove(i)

    return arg_list


def check_occurs(var, expr):
    arg_list = get_arg_list(expr)
    if var in arg_list:
        return True

    return False


def unify(expr1, expr2):

    if is_variable(expr1) and is_variable(expr2):
        if expr1 == expr2:
            return 'Null'
        else:
            return False
    elif is_variable(expr1) and not is_variable(expr2):
        if check_occurs(expr1, expr2):
            return False
        else:
            tmp = str(expr2) + '/' + str(expr1)
            return tmp
    elif not is_variable(expr1) and is_variable(expr2):
        if check_occurs(expr2, expr1):
            return False
        else:
            tmp = str(expr1) + '/' + str(expr2)
            return tmp
    else:
        predicate_symbol_1, arg_list_1 = process_expression(expr1)
        predicate_symbol_2, arg_list_2 = process_expression(expr2)

        # Step 2
        if predicate_symbol_1 != predicate_symbol_2:
            return False
        # Step 3
        elif len(arg_list_1) != len(arg_list_2):
            return False
        else:
            # Step 4: Create substitution list
            sub_list = list()

            # Step 5:
            for i in range(len(arg_list_1)):
                tmp = unify(arg_list_1[i], arg_list_2[i])

                if not tmp:
                    return False
                elif tmp == 'Null':
                    pass
                else:
                    if type(tmp) == list:
                        for j in tmp:
                            sub_list.append(j)
                    else:
                        sub_list.append(tmp)

            # Step 6
            return sub_list


if __name__ == '__main__':
    
    f1 = 'Q(a, g(x, a), f(y))'
    f2 = 'Q(a, g(f(b), a), x)'
    # f1 = input('f1 : ')
    # f2 = input('f2 : ')

    result = unify(f1, f2)
    if not result:
        print('The process of Unification failed!')
    else:
        print('The process of Unification successful!')
        print(result)

###Resolution


import copy
import time


class Parameter:
    variable_count = 1

    def __init__(self, name=None):
        if name:
            self.type = "Constant"
            self.name = name
        else:
            self.type = "Variable"
            self.name = "v" + str(Parameter.variable_count)
            Parameter.variable_count += 1

    def isConstant(self):
        return self.type == "Constant"

    def unify(self, type_, name):
        self.type = type_
        self.name = name

    def __eq__(self, other):
        return self.name == other.name

    def __str__(self):
        return self.name


class Predicate:
    def __init__(self, name, params):
        self.name = name
        self.params = params

    def __eq__(self, other):
        return self.name == other.name and all(a == b for a, b in zip(self.params, other.params))

    def __str__(self):
        return self.name + "(" + ",".join(str(x) for x in self.params) + ")"

    def getNegatedPredicate(self):
        return Predicate(negatePredicate(self.name), self.params)


class Sentence:
    sentence_count = 0

    def __init__(self, string):
        self.sentence_index = Sentence.sentence_count
        Sentence.sentence_count += 1
        self.predicates = []
        self.variable_map = {}
        local = {}

        for predicate in string.split("|"):
            name = predicate[:predicate.find("(")]
            params = []

            for param in predicate[predicate.find("(") + 1: predicate.find(")")].split(","):
                if param[0].islower():
                    if param not in local:  # Variable
                        local[param] = Parameter()
                        self.variable_map[local[param].name] = local[param]
                    new_param = local[param]
                else:
                    new_param = Parameter(param)
                    self.variable_map[param] = new_param

                params.append(new_param)

            self.predicates.append(Predicate(name, params))

    def getPredicates(self):
        return [predicate.name for predicate in self.predicates]

    def findPredicates(self, name):
        return [predicate for predicate in self.predicates if predicate.name == name]

    def removePredicate(self, predicate):
        self.predicates.remove(predicate)
        for key, val in self.variable_map.items():
            if not val:
                self.variable_map.pop(key)

    def containsVariable(self):
        return any(not param.isConstant() for param in self.variable_map.values())

    def __eq__(self, other):
        if len(self.predicates) == 1 and self.predicates[0] == other:
            return True
        return False

    def __str__(self):
        return "".join([str(predicate) for predicate in self.predicates])


class KB:
    def __init__(self, inputSentences):
        self.inputSentences = [x.replace(" ", "") for x in inputSentences]
        self.sentences = []
        self.sentence_map = {}

    def prepareKB(self):
        self.convertSentencesToCNF()
        for sentence_string in self.inputSentences:
            sentence = Sentence(sentence_string)
            for predicate in sentence.getPredicates():
                self.sentence_map[predicate] = self.sentence_map.get(
                    predicate, []) + [sentence]

    def convertSentencesToCNF(self):
        for sentenceIdx in range(len(self.inputSentences)):
            # Do negation of the Premise and add them as literal
            if "=>" in self.inputSentences[sentenceIdx]:
                self.inputSentences[sentenceIdx] = negateAntecedent(
                    self.inputSentences[sentenceIdx])

    def askQueries(self, queryList):
        results = []

        for query in queryList:
            negatedQuery = Sentence(negatePredicate(query.replace(" ", "")))
            negatedPredicate = negatedQuery.predicates[0]
            prev_sentence_map = copy.deepcopy(self.sentence_map)
            self.sentence_map[negatedPredicate.name] = self.sentence_map.get(
                negatedPredicate.name, []) + [negatedQuery]
            self.timeLimit = time.time() + 40

            try:
                result = self.resolve([negatedPredicate], [
                                      False]*(len(self.inputSentences) + 1))
            except:
                result = False

            self.sentence_map = prev_sentence_map

            if result:
                results.append("TRUE")
            else:
                results.append("FALSE")

        return results

    def resolve(self, queryStack, visited, depth=0):
        if time.time() > self.timeLimit:
            raise Exception
        if queryStack:
            query = queryStack.pop(-1)
            negatedQuery = query.getNegatedPredicate()
            queryPredicateName = negatedQuery.name
            if queryPredicateName not in self.sentence_map:
                return False
            else:
                queryPredicate = negatedQuery
                for kb_sentence in self.sentence_map[queryPredicateName]:
                    if not visited[kb_sentence.sentence_index]:
                        for kbPredicate in kb_sentence.findPredicates(queryPredicateName):

                            canUnify, substitution = performUnification(
                                copy.deepcopy(queryPredicate), copy.deepcopy(kbPredicate))

                            if canUnify:
                                newSentence = copy.deepcopy(kb_sentence)
                                newSentence.removePredicate(kbPredicate)
                                newQueryStack = copy.deepcopy(queryStack)

                                if substitution:
                                    for old, new in substitution.items():
                                        if old in newSentence.variable_map:
                                            parameter = newSentence.variable_map[old]
                                            newSentence.variable_map.pop(old)
                                            parameter.unify(
                                                "Variable" if new[0].islower() else "Constant", new)
                                            newSentence.variable_map[new] = parameter

                                    for predicate in newQueryStack:
                                        for index, param in enumerate(predicate.params):
                                            if param.name in substitution:
                                                new = substitution[param.name]
                                                predicate.params[index].unify(
                                                    "Variable" if new[0].islower() else "Constant", new)

                                for predicate in newSentence.predicates:
                                    newQueryStack.append(predicate)

                                new_visited = copy.deepcopy(visited)
                                if kb_sentence.containsVariable() and len(kb_sentence.predicates) > 1:
                                    new_visited[kb_sentence.sentence_index] = True

                                if self.resolve(newQueryStack, new_visited, depth + 1):
                                    return True
                return False
        return True


def performUnification(queryPredicate, kbPredicate):
    substitution = {}
    if queryPredicate == kbPredicate:
        return True, {}
    else:
        for query, kb in zip(queryPredicate.params, kbPredicate.params):
            if query == kb:
                continue
            if kb.isConstant():
                if not query.isConstant():
                    if query.name not in substitution:
                        substitution[query.name] = kb.name
                    elif substitution[query.name] != kb.name:
                        return False, {}
                    query.unify("Constant", kb.name)
                else:
                    return False, {}
            else:
                if not query.isConstant():
                    if kb.name not in substitution:
                        substitution[kb.name] = query.name
                    elif substitution[kb.name] != query.name:
                        return False, {}
                    kb.unify("Variable", query.name)
                else:
                    if kb.name not in substitution:
                        substitution[kb.name] = query.name
                    elif substitution[kb.name] != query.name:
                        return False, {}
    return True, substitution


def negatePredicate(predicate):
    return predicate[1:] if predicate[0] == "~" else "~" + predicate


def negateAntecedent(sentence):
    antecedent = sentence[:sentence.find("=>")]
    premise = []

    for predicate in antecedent.split("&"):
        premise.append(negatePredicate(predicate))

    premise.append(sentence[sentence.find("=>") + 2:])
    return "|".join(premise)


def getInput(filename):
    with open(filename, "r") as file:
        noOfQueries = int(file.readline().strip())
        inputQueries = [file.readline().strip() for _ in range(noOfQueries)]
        noOfSentences = int(file.readline().strip())
        inputSentences = [file.readline().strip()
                          for _ in range(noOfSentences)]
        return inputQueries, inputSentences


def printOutput(filename, results):
    print(results)
    with open(filename, "w") as file:
        for line in results:
            file.write(line)
            file.write("\n")
    file.close()


if __name__ == '__main__':
    inputQueries_, inputSentences_ = getInput('input_1.txt')
    knowledgeBase = KB(inputSentences_)
    knowledgeBase.prepareKB()
    results_ = knowledgeBase.askQueries(inputQueries_)
    printOutput("output.txt", results_)



"""**EXP-8 LEARNING ALGORITHM**

Logistic Regression
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

dataset = pd.read_csv('Social_Network_Ads.csv')
X = dataset.iloc[:, [2, 3]].values
y = dataset.iloc[:, 4].values

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression()
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

from sklearn import metrics
print("Logistic Regression model accuracy(in %):", metrics.accuracy_score(y_test, y_pred)*100)

"""SVM"""

# Support Vector Machine
# Importing the libraries

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the datasets

datasets = pd.read_csv('Social_Network_Ads.csv')
X = datasets.iloc[:, [2,3]].values
Y = datasets.iloc[:, 4].values

# Splitting the dataset into the Training set and Test set

from sklearn.model_selection import train_test_split
X_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size = 0.25, random_state = 0)

# Feature Scaling

from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_Train = sc_X.fit_transform(X_Train)
X_Test = sc_X.transform(X_Test)

# Fitting the classifier into the Training set

from sklearn.svm import SVC
classifier = SVC(kernel = 'linear', random_state = 0)
classifier.fit(X_Train, Y_Train)

# Predicting the test set results

Y_Pred = classifier.predict(X_Test)

# Making the Confusion Matrix 

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(Y_Test, Y_Pred)

# Visualising the Training set results

from matplotlib.colors import ListedColormap
X_Set, Y_Set = X_Train, Y_Train
X1, X2 = np.meshgrid(np.arange(start = X_Set[:, 0].min() - 1, stop = X_Set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_Set[:, 1].min() - 1, stop = X_Set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(Y_Set)):
    plt.scatter(X_Set[Y_Set == j, 0], X_Set[Y_Set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Support Vector Machine (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

# Visualising the Test set results

from matplotlib.colors import ListedColormap
X_Set, Y_Set = X_Test, Y_Test
X1, X2 = np.meshgrid(np.arange(start = X_Set[:, 0].min() - 1, stop = X_Set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_Set[:, 1].min() - 1, stop = X_Set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(Y_Set)):
    plt.scatter(X_Set[Y_Set == j, 0], X_Set[Y_Set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Support Vector Machine (Test set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

"""Naive Bayes"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import seaborn as sns

iris=pd.read_csv('Iris.csv')

iris.head()

iris['Species'].unique()

iris.info()

iris.drop(columns="Id",inplace=True)

iris.isnull().sum()

import missingno as msno
msno.bar(iris,figsize=(8,6),color='skyblue')
plt.show()

"""ScatterPlot"""

g=sns.relplot(x='SepalLengthCm',y='SepalWidthCm',data=iris,hue='Species',style='Species')
g.fig.set_size_inches(10,5)
plt.show()

g=sns.relplot(x='PetalLengthCm',y='PetalWidthCm',data=iris,hue='Species',style='Species')
g.fig.set_size_inches(10,5)
plt.show()

sns.pairplot(iris,hue="Species")
plt.show()

#Metrics
from sklearn.metrics import make_scorer, accuracy_score,precision_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score ,precision_score,recall_score,f1_score

#Model Select
from sklearn.model_selection import KFold,train_test_split,cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.linear_model import  LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn import linear_model
from sklearn.linear_model import SGDClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC, LinearSVC
from sklearn.naive_bayes import GaussianNB

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=0)

gaussian = GaussianNB()
gaussian.fit(X_train, y_train)
Y_pred = gaussian.predict(X_test) 
accuracy_nb=round(accuracy_score(y_test,Y_pred)* 100, 2)
acc_gaussian = round(gaussian.score(X_train, y_train) * 100, 2)

cm = confusion_matrix(y_test, Y_pred)
accuracy = accuracy_score(y_test,Y_pred)
precision =precision_score(y_test, Y_pred,average='micro')
recall =  recall_score(y_test, Y_pred,average='micro')
f1 = f1_score(y_test,Y_pred,average='micro')
print('Confusion matrix for Naive Bayes\n',cm)
print('accuracy_Naive Bayes: %.3f' %accuracy)
print('precision_Naive Bayes: %.3f' %precision)
print('recall_Naive Bayes: %.3f' %recall)
print('f1-score_Naive Bayes : %.3f' %f1)

"""Natural Language Processing"""

# Import necessary libraries
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.sentiment import SentimentIntensityAnalyzer

# Download necessary resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('vader_lexicon')

# Define a sample text
text = "Natural Language Processing (NLP) is a subfield of artificial intelligence concerned with the interaction between computers and humans in natural language."

# Tokenize the text
tokens = word_tokenize(text)

# Remove stopwords
stop_words = set(stopwords.words('english'))
filtered_tokens = [token for token in tokens if token.lower() not in stop_words]

# Perform stemming
stemmer = PorterStemmer()
stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]

# Perform sentiment analysis
analyzer = SentimentIntensityAnalyzer()
sentiment_score = analyzer.polarity_scores(text)

# Print the results
print("Original text: ", text)
print("Tokenized text: ", tokens)
print("Filtered tokens: ", filtered_tokens)
print("Stemmed tokens: ", stemmed_tokens)
print("Sentiment score: ", sentiment_score)

print("Tokenized text: ", tokens)

print("Filtered tokens: ", filtered_tokens)

print("Stemmed tokens: ", stemmed_tokens)

print("Sentiment score: ", sentiment_score)



"""## **Exp 10: Appling Deep Learning**"""

import tensorflow as tf
from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt
import numpy as np

(X_train, y_train), (X_test,y_test) = datasets.cifar10.load_data()
X_train.shape

X_test.shape

y_train.shape

y_train[:5]

y_train = y_train.reshape(-1,)
y_train[:5]

classes = ["airplane","automobile","bird","cat","deer","dog","frog","horse","ship","truck"]

def plot_sample(X, y, index):
    plt.figure(figsize = (15,2))
    plt.imshow(X[index])
    plt.xlabel(classes[y[index]])

plot_sample(X_train, y_train, 0)

plot_sample(X_train, y_train, 1)

#normalizing the data
X_train = X_train / 255.0
X_test = X_test / 255.0

#building a dataset
ann = models.Sequential([
        layers.Flatten(input_shape=(32,32,3)),
        layers.Dense(3000, activation='relu'),
        layers.Dense(1000, activation='relu'),
        layers.Dense(10, activation='softmax')    
    ])

ann.compile(optimizer='SGD',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

ann.fit(X_train, y_train, epochs=5)

#classification report
from sklearn.metrics import confusion_matrix , classification_report
import numpy as np
y_pred = ann.predict(X_test)
y_pred_classes = [np.argmax(element) for element in y_pred]

print("Classification Report: \n", classification_report(y_test, y_pred_classes))

cnn = models.Sequential([
    layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)),
    layers.MaxPooling2D((2, 2)),
    
    layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')
])

cnn.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

cnn.fit(X_train, y_train, epochs=10)

from sklearn.metrics import confusion_matrix , classification_report
import numpy as np
y_pred = cnn.predict(X_test)
y_pred_classes = [np.argmax(element) for element in y_pred]

print("Classification Report: \n", classification_report(y_test, y_pred_classes))







!git clone https://github.com/Aryaman047/AI-Lab

!git pull

!git pull https://github.com/Aryaman047/AI-Lab

!git pull https://github.com/Aryaman047/AI-Lab.git

